{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Text Embedding Generator with PLaMo-1B\n",
    "\n",
    "**Goal:** This notebook demonstrates how to generate high-quality text embeddings for Japanese sentences using the `pfnet/plamo-embedding-1b` model from Hugging Face. Embeddings are numerical representations of text that capture semantic meaning, useful for various NLP tasks like similarity search, clustering, and classification.\n",
    "\n",
    "We will cover:\n",
    "1.  Installing necessary libraries.\n",
    "2.  Loading the pre-trained PLaMo model and its tokenizer.\n",
    "3.  Defining a function to generate embeddings using mean pooling.\n",
    "4.  Running examples for single sentences and batches of sentences.\n",
    "5.  Saving the generated embeddings to a file and loading them back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Installation\n",
    "\n",
    "The following libraries are required:\n",
    "*   `torch`: The core PyTorch library for tensor computations and neural network operations.\n",
    "*   `transformers`: Hugging Face's library providing access to pre-trained models (like PLaMo) and tokenizers.\n",
    "*   `sentencepiece`: A tokenizer library often used by models like PLaMo.\n",
    "*   `numpy`: A library for numerical operations, especially for handling the embeddings as arrays.\n",
    "\n",
    "The cell below will install these libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers sentencepiece numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Now, let's import the installed libraries and necessary modules.\n",
    "*   `torch` for tensor operations.\n",
    "*   `AutoTokenizer` and `AutoModel` from `transformers` for loading the model and tokenizer automatically based on the model name.\n",
    "*   `numpy` (as `np`) for numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model and Tokenizer\n",
    "\n",
    "We will use the `pfnet/plamo-embedding-1b` model, a powerful model specifically trained for generating Japanese text embeddings.\n",
    "\n",
    "*   **`MODEL_NAME`**: Specifies the Hugging Face model identifier.\n",
    "*   **`AutoTokenizer.from_pretrained(MODEL_NAME)`**: Loads the appropriate tokenizer for the specified model. The tokenizer converts text into a format (token IDs, attention masks) that the model can understand.\n",
    "*   **`AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)`**: Loads the pre-trained PLaMo model.\n",
    "    *   `trust_remote_code=True`: This argument is sometimes required for models that have custom code within their Hugging Face repository. It allows the execution of this custom code. Always ensure you trust the source of the model when using this option.\n",
    "*   **`device`**: We check if a CUDA-enabled GPU is available and set the device accordingly (`'cuda'` or `'cpu'`).\n",
    "*   **`.to(device)`**: This moves the model's parameters and buffers to the selected device (GPU if available, otherwise CPU). Processing on a GPU significantly speeds up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "MODEL_NAME = 'pfnet/plamo-embedding-1b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# It's good practice to move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation Function\n",
    "\n",
    "The function `get_japanese_embedding` takes either a single Japanese sentence or a list of sentences and returns their embeddings.\n",
    "\n",
    "**Function Breakdown:**\n",
    "\n",
    "1.  **Tokenization (`tokenizer(...)`):**\n",
    "    *   `text_or_texts`: The input Japanese string or list of strings.\n",
    "    *   `return_tensors='pt'`: Returns PyTorch tensors.\n",
    "    *   `truncation=True`: Truncates sequences to the model's maximum input length if they are too long.\n",
    "    *   `padding=True`: Pads shorter sequences to the length of the longest sequence in a batch, ensuring uniform tensor dimensions.\n",
    "    *   `max_length=512`: Sets the maximum sequence length. (Note: PLaMo-1B's default is 2048. For many common uses, 512 is a practical starting point, but this can be adjusted. The model will handle sequences up to its configured maximum.)\n",
    "    *   `add_special_tokens=True`: Adds special tokens like `[CLS]` and `[SEP]` if the model expects them (PLaMo does).\n",
    "    *   `.to(device)`: Moves the tokenized inputs to the same device as the model.\n",
    "\n",
    "2.  **Model Inference (`with torch.no_grad(): ... outputs = model(**inputs)`)**\n",
    "    *   `torch.no_grad()`: Disables gradient calculations, which is crucial for inference as it reduces memory consumption and speeds up computations when we are not training the model.\n",
    "    *   `model(**inputs)`: Passes the tokenized input (input IDs, attention mask, etc.) to the model. The model returns a set of outputs, including the `last_hidden_state`.\n",
    "\n",
    "3.  **Mean Pooling for Sentence Embedding:**\n",
    "    *   `last_hidden_states = outputs.last_hidden_state`: This tensor contains the embeddings for each token in the input sequence(s). Its shape is typically (batch_size, sequence_length, hidden_dim).\n",
    "    *   **Attention Mask (`inputs['attention_mask']`)**: The attention mask is used to distinguish real tokens from padding tokens. It has a value of 1 for real tokens and 0 for padding tokens.\n",
    "    *   `expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()`: The attention mask is expanded to match the dimensions of `last_hidden_states`. This allows element-wise multiplication to zero out the embeddings of padding tokens.\n",
    "    *   `sum_embeddings = torch.sum(last_hidden_states * expanded_mask, 1)`: The embeddings of non-padding tokens are summed up along the sequence dimension (dimension 1).\n",
    "    *   `sum_mask = expanded_mask.sum(1)`: The number of actual (non-padding) tokens in each sequence is calculated.\n",
    "    *   `mean_embeddings = sum_embeddings / sum_mask`: The sum of embeddings is divided by the number of actual tokens to get the mean embedding. This mean-pooled embedding represents the entire sentence.\n",
    "    *   `clamp(sum_mask, min=1e-9)`: Prevents division by zero if a sequence had no actual tokens (though padding and tokenizer settings usually prevent this).\n",
    "\n",
    "4.  **Output:**\n",
    "    *   `.cpu().numpy().squeeze()`: The resulting embeddings are moved to the CPU, converted to a NumPy array, and any unnecessary single dimensions (e.g., if a single sentence was input) are removed using `squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings using mean pooling\n",
    "def get_japanese_embedding(text_or_texts):\n",
    "    # Tokenize the text (handles single string or list of strings)\n",
    "    inputs = tokenizer(text_or_texts, return_tensors='pt', truncation=True, padding=True, max_length=512, add_special_tokens=True).to(device)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Perform mean pooling\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_states * expanded_mask, 1)\n",
    "    sum_mask = expanded_mask.sum(1)\n",
    "    sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    # Squeeze is important here: if input was single string, result is (1, dim), squeeze to (dim,)\n",
    "    # If input was list of N strings, result is (N, dim), squeeze does nothing if N > 1.\n",
    "    return mean_embeddings.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Usage\n",
    "\n",
    "The following cell demonstrates how to use the `get_japanese_embedding` function.\n",
    "It shows:\n",
    "1.  Generating an embedding for a single Japanese sentence.\n",
    "2.  Generating embeddings for a batch of multiple Japanese sentences.\n",
    "    *   The function `get_japanese_embedding` handles both single strings and lists of strings automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage for generating embeddings\n",
    "print(f\"Using Model: {MODEL_NAME}\\n\")\n",
    "\n",
    "# 1. Single sentence example\n",
    "print(\"--- Single Sentence Embedding ---\")\n",
    "sample_text_single = \"こんにちは、美しい世界！\"\n",
    "embedding_single = get_japanese_embedding(sample_text_single)\n",
    "print(f\"Original sentence: {sample_text_single}\")\n",
    "print(f\"Embedding shape: {embedding_single.shape}\")\n",
    "print(f\"Sample embedding (first 5 values): {embedding_single[:5]}\\n\")\n",
    "\n",
    "# 2. Batch (multiple sentences) example\n",
    "print(\"--- Batch Sentences Embedding ---\")\n",
    "sample_texts_batch = [\n",
    "    \"これは最初の文です。\",\n",
    "    \"日本語の埋め込みをテストしています。\",\n",
    "    \"これが最後の文になります。\"\n",
    "]\n",
    "embeddings_batch = get_japanese_embedding(sample_texts_batch)\n",
    "print(f\"Original sentences: {sample_texts_batch}\")\n",
    "print(f\"Batch embeddings shape: {embeddings_batch.shape}\") # Should be (num_sentences, embedding_dim)\n",
    "if embeddings_batch.ndim == 2 and embeddings_batch.shape[0] > 0:\n",
    "    print(f\"Sample embedding for the first sentence (first 5 values): {embeddings_batch[0, :5]}\\n\")\n",
    "else:\n",
    "    print(\"Could not display sample of batch embeddings due to unexpected shape.\\n\")"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 6. Saving and Loading Embeddings\n",
      "\n",
      "After generating embeddings, it's often useful to save them for later use, avoiding the need to recompute them. NumPy's `.npy` format is efficient for storing numerical arrays.\n",
      "\n",
      "*   **`np.save(filename, array)`**: Saves the NumPy array (in our case, `embeddings_batch`) to the specified file (e.g., `japanese_embeddings.npy`).\n",
      "*   **`np.load(filename)`**: Loads the array back from the `.npy` file.\n",
      "\n",
      "The following code demonstrates this process, using the `embeddings_batch` generated in the previous cell."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and Loading Example (uses embeddings_batch from the cell above)\n",
    "\n",
    "# Check if embeddings_batch exists and has content (e.g. if previous cell was run)\n",
    "if 'embeddings_batch' in locals() and embeddings_batch.size > 0:\n",
    "    # 3. Saving batch embeddings to a .npy file\n",
    "    print(\"--- Saving Batch Embeddings ---\")\n",
    "    output_filename = \"japanese_embeddings.npy\"\n",
    "    np.save(output_filename, embeddings_batch)\n",
    "    print(f\"Batch embeddings saved to: {output_filename}\\n\")\n",
    "\n",
    "    # 4. Loading embeddings from the .npy file\n",
    "    print(\"--- Loading Batch Embeddings ---\")\n",
    "    loaded_embeddings = np.load(output_filename)\n",
    "    print(f\"Embeddings loaded from: {output_filename}\")\n",
    "    print(f\"Loaded embeddings shape: {loaded_embeddings.shape}\")\n",
    "    if loaded_embeddings.ndim == 2 and loaded_embeddings.shape[0] > 0:\n",
    "        print(f\"Sample of loaded embedding for the first sentence (first 5 values): {loaded_embeddings[0, :5]}\")\n",
    "else:\n",
    "    print(\"Skipping saving/loading example as 'embeddings_batch' is not defined or empty. Please run the cell above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
