{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b61b89",
   "metadata": {},
   "source": [
    "# Japanese Text Embedding Generator with PLaMo-1B\n",
    "\n",
    "**Goal:** This notebook demonstrates how to generate high-quality text embeddings for Japanese sentences using the `pfnet/plamo-embedding-1b` model from Hugging Face. Embeddings are numerical representations of text that capture semantic meaning, useful for various NLP tasks like similarity search, clustering, and classification.\n",
    "\n",
    "We will cover:\n",
    "1.  Installing necessary libraries.\n",
    "2.  Loading the pre-trained PLaMo model and its tokenizer.\n",
    "3.  Defining a function to generate embeddings using mean pooling.\n",
    "4.  Defining a utility function for cosine similarity (NumPy-based).\n",
    "5.  Defining a function for semantic search (using PyTorch-based cosine similarity).\n",
    "6.  Running examples for single sentences, batches, saving/loading, and search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dae30a",
   "metadata": {},
   "source": [
    "## 1. Library Installation\n",
    "\n",
    "The following libraries are required:\n",
    "*   `torch`: The core PyTorch library for tensor computations and neural network operations.\n",
    "*   `transformers`: Hugging Face's library providing access to pre-trained models (like PLaMo) and tokenizers.\n",
    "*   `sentencepiece`: A tokenizer library often used by models like PLaMo.\n",
    "*   `numpy`: A library for numerical operations, especially for handling the embeddings as arrays.\n",
    "\n",
    "The cell below will install these libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4daaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers sentencepiece numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9df08e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Now, let's import the installed libraries and necessary modules.\n",
    "*   `torch` for tensor operations.\n",
    "*   `torch.nn.functional` (as `F`) for PyTorch functions like cosine similarity.\n",
    "*   `AutoTokenizer` and `AutoModel` from `transformers` for loading the model and tokenizer automatically based on the model name.\n",
    "*   `numpy` (as `np`) for numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee923a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076e822",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model and Tokenizer\n",
    "\n",
    "We will use the `pfnet/plamo-embedding-1b` model, a powerful model specifically trained for generating Japanese text embeddings.\n",
    "\n",
    "*   **`MODEL_NAME`**: Specifies the Hugging Face model identifier.\n",
    "*   **`AutoTokenizer.from_pretrained(MODEL_NAME)`**: Loads the appropriate tokenizer for the specified model. The tokenizer converts text into a format (token IDs, attention masks) that the model can understand.\n",
    "*   **`AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)`**: Loads the pre-trained PLaMo model.\n",
    "    *   `trust_remote_code=True`: This argument is sometimes required for models that have custom code within their Hugging Face repository. It allows the execution of this custom code. Always ensure you trust the source of the model when using this option.\n",
    "*   **`device`**: We check if a CUDA-enabled GPU is available and set the device accordingly (`'cuda'` or `'cpu'`).\n",
    "*   **`.to(device)`**: This moves the model's parameters and buffers to the selected device (GPU if available, otherwise CPU). Processing on a GPU significantly speeds up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16773abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "MODEL_NAME = 'pfnet/plamo-embedding-1b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# It's good practice to move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d38dbd",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation Function\n",
    "\n",
    "The function `get_japanese_embedding` takes either a single Japanese sentence or a list of sentences and returns their embeddings as NumPy arrays.\n",
    "\n",
    "**Function Breakdown:**\n",
    "\n",
    "1.  **Tokenization (`tokenizer(...)`):**\n",
    "    *   `text_or_texts`: The input Japanese string or list of strings.\n",
    "    *   `return_tensors='pt'`: Returns PyTorch tensors.\n",
    "    *   `truncation=True`: Truncates sequences to the model's maximum input length if they are too long.\n",
    "    *   `padding=True`: Pads shorter sequences to the length of the longest sequence in a batch, ensuring uniform tensor dimensions.\n",
    "    *   `max_length=512`: Sets the maximum sequence length. (Note: PLaMo-1B's default is 2048. For many common uses, 512 is a practical starting point, but this can be adjusted. The model will handle sequences up to its configured maximum.)\n",
    "    *   `add_special_tokens=True`: Adds special tokens like `[CLS]` and `[SEP]` if the model expects them (PLaMo does).\n",
    "    *   `.to(device)`: Moves the tokenized inputs to the same device as the model.\n",
    "\n",
    "2.  **Model Inference (`with torch.no_grad(): ... outputs = model(**inputs)`)**\n",
    "    *   `torch.no_grad()`: Disables gradient calculations, which is crucial for inference as it reduces memory consumption and speeds up computations when we are not training the model.\n",
    "    *   `model(**inputs)`: Passes the tokenized input (input IDs, attention mask, etc.) to the model. The model returns a set of outputs, including the `last_hidden_state`.\n",
    "\n",
    "3.  **Mean Pooling for Sentence Embedding:**\n",
    "    *   `last_hidden_states = outputs.last_hidden_state`: This tensor contains the embeddings for each token in the input sequence(s). Its shape is typically (batch_size, sequence_length, hidden_dim).\n",
    "    *   **Attention Mask (`inputs['attention_mask']`)**: The attention mask is used to distinguish real tokens from padding tokens. It has a value of 1 for real tokens and 0 for padding tokens.\n",
    "    *   `expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()`: The attention mask is expanded to match the dimensions of `last_hidden_states`. This allows element-wise multiplication to zero out the embeddings of padding tokens.\n",
    "    *   `sum_embeddings = torch.sum(last_hidden_states * expanded_mask, 1)`: The embeddings of non-padding tokens are summed up along the sequence dimension (dimension 1).\n",
    "    *   `sum_mask = expanded_mask.sum(1)`: The number of actual (non-padding) tokens in each sequence is calculated.\n",
    "    *   `mean_embeddings = sum_embeddings / sum_mask`: The sum of embeddings is divided by the number of actual tokens to get the mean embedding. This mean-pooled embedding represents the entire sentence.\n",
    "    *   `clamp(sum_mask, min=1e-9)`: Prevents division by zero if a sequence had no actual tokens (though padding and tokenizer settings usually prevent this).\n",
    "\n",
    "4.  **Output:**\n",
    "    *   `.cpu().numpy().squeeze()`: The resulting embeddings are moved to the CPU, converted to a NumPy array, and any unnecessary single dimensions (e.g., if a single sentence was input) are removed using `squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings using mean pooling\n",
    "def get_japanese_embedding(text_or_texts):\n",
    "    # Tokenize the text (handles single string or list of strings)\n",
    "    inputs = tokenizer(text_or_texts, return_tensors='pt', truncation=True, padding=True, max_length=512, add_special_tokens=True).to(device)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Perform mean pooling\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_states * expanded_mask, 1)\n",
    "    sum_mask = expanded_mask.sum(1)\n",
    "    sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    # Squeeze is important here: if input was single string, result is (1, dim), squeeze to (dim,)\n",
    "    # If input was list of N strings, result is (N, dim), squeeze does nothing if N > 1.\n",
    "    return mean_embeddings.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de3cf8",
   "metadata": {},
   "source": [
    "## Utility Function: Cosine Similarity (NumPy-based)\n",
    "\n",
    "The following function calculates the cosine similarity between two vectors (or a vector and a matrix of vectors) using NumPy. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. It's widely used to compare document or sentence embeddings.\n",
    "\n",
    "- If `vec_a` is a 1D array and `vec_b` is a 1D array, it returns a single similarity score.\n",
    "- If `vec_a` is a 1D array and `vec_b` is a 2D array (matrix), it calculates the cosine similarity between `vec_a` and each row vector in `vec_b`, returning a 1D array of similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e256902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NumPy is imported as np in cell 4)\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between vec_a and vec_b using NumPy.\n",
    "    vec_a: 1D NumPy array.\n",
    "    vec_b: 1D or 2D NumPy array. If 2D, similarity is computed between vec_a and each row of vec_b.\n",
    "    Returns: A scalar float if vec_b was 1D, or a 1D NumPy array of floats if vec_b was 2D.\n",
    "    \"\"\"\n",
    "    vec_a_norm = np.linalg.norm(vec_a)\n",
    "    vec_b_norms = np.linalg.norm(vec_b, axis=-1) # works for 1D and 2D (along last axis)\n",
    "\n",
    "    if vec_a_norm == 0: # If vec_a is zero vector, similarity is 0\n",
    "        return 0.0 if vec_b.ndim == 1 else np.zeros(vec_b.shape[0])\n",
    "            \n",
    "    if vec_b.ndim == 1:\n",
    "        dot_prod = np.dot(vec_a, vec_b)\n",
    "        if vec_b_norms == 0: return 0.0 # If vec_b is zero vector\n",
    "        similarities = dot_prod / (vec_a_norm * vec_b_norms)\n",
    "    elif vec_b.ndim == 2:\n",
    "        dot_prod = np.dot(vec_b, vec_a) # (N,D) @ (D,) -> (N,)\n",
    "        # Initialize similarities to zero, especially for cases where vec_b_norms might be zero\n",
    "        similarities = np.zeros(vec_b.shape[0])\n",
    "        # Calculate similarities only for rows with non-zero norm\n",
    "        non_zero_mask = (vec_b_norms != 0)\n",
    "        if np.any(non_zero_mask):\n",
    "             similarities[non_zero_mask] = dot_prod[non_zero_mask] / (vec_a_norm * vec_b_norms[non_zero_mask])\n",
    "    else:\n",
    "        raise ValueError(\"vec_b must be 1D or 2D array\")\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc69800",
   "metadata": {},
   "source": [
    "## Semantic Search Function (using PyTorch Cosine Similarity)\n",
    "\n",
    "The `search_similar_texts` function implements semantic search. Given a query text, it finds the most similar texts from a provided list of original texts by comparing their embeddings.\n",
    "\n",
    "**Function Parameters:**\n",
    "*   `query_text` (str): The Japanese text to search with.\n",
    "*   `original_texts` (list of str): A list of the original Japanese sentences/texts that correspond to `all_embeddings`.\n",
    "*   `all_embeddings` (2D NumPy array): A NumPy array where each row is an embedding of a text from `original_texts`. Assumes these embeddings were pre-calculated using `get_japanese_embedding`.\n",
    "*   `top_n` (int, optional): The number of top similar texts to return. Defaults to 3.\n",
    "\n",
    "**Process:**\n",
    "1.  Generates an embedding for the `query_text` (as a NumPy array) using the `get_japanese_embedding` function.\n",
    "2.  Converts the NumPy query embedding and the `all_embeddings` NumPy array to PyTorch tensors, moving them to the active `device`.\n",
    "3.  Calculates the cosine similarities between the query tensor and the tensor of all embeddings. (Note: This implementation uses `torch.nn.functional.cosine_similarity` for efficient computation on tensors).\n",
    "4.  Converts the resulting similarity tensor back to a NumPy array.\n",
    "5.  Identifies the `top_n` texts with the highest similarity scores using NumPy for sorting.\n",
    "6.  Returns a list of tuples, where each tuple contains `(similar_text, similarity_score)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NumPy 'np', 'get_japanese_embedding' are defined in previous cells)\n",
    "# (PyTorch 'torch', 'device', 'F' are defined/imported in previous cells)\n",
    "\n",
    "def search_similar_texts(query_text, original_texts, all_embeddings, top_n=3):\n",
    "    \"\"\"\n",
    "    Searches for texts in 'original_texts' that are semantically similar to 'query_text'.\n",
    "    Uses torch.nn.functional.cosine_similarity for the calculation.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text to search for.\n",
    "        original_texts (list): A list of original text strings.\n",
    "        all_embeddings (np.ndarray): A 2D NumPy array of embeddings corresponding to original_texts.\n",
    "        top_n (int): Number of top similar texts to return.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples, each containing (similar_text, similarity_score), \n",
    "              sorted by similarity in descending order.\n",
    "    \"\"\"\n",
    "    if not isinstance(all_embeddings, np.ndarray):\n",
    "        all_embeddings = np.array(all_embeddings)\n",
    "    if all_embeddings.ndim != 2:\n",
    "        raise ValueError(f\"all_embeddings must be a 2D array, but got {all_embeddings.ndim} dimensions.\")\n",
    "    if len(original_texts) != all_embeddings.shape[0]:\n",
    "        raise ValueError(f\"Number of original_texts ({len(original_texts)}) must match number of rows in all_embeddings ({all_embeddings.shape[0]}).\")\n",
    "\n",
    "    # 1. Get query embedding (NumPy array from get_japanese_embedding)\n",
    "    query_embedding_np = get_japanese_embedding(query_text)\n",
    "    if query_embedding_np.ndim == 2: # Ensure it's 1D for a single query\n",
    "        query_embedding_np = query_embedding_np.squeeze(0)\n",
    "\n",
    "    # 2. Convert NumPy embeddings to PyTorch tensors and move to device\n",
    "    query_tensor = torch.from_numpy(query_embedding_np).unsqueeze(0).to(device) # Shape: (1, D)\n",
    "    all_embeddings_tensor = torch.from_numpy(all_embeddings).to(device)      # Shape: (N, D)\n",
    "\n",
    "    # 3. Calculate cosine similarities using torch.nn.functional\n",
    "    # query_tensor (1,D) is broadcast against all_embeddings_tensor (N,D)\n",
    "    # dim=-1 ensures similarity is computed along the embedding dimension\n",
    "    similarities_tensor = F.cosine_similarity(query_tensor, all_embeddings_tensor, dim=-1) # Output shape: (N,)\n",
    "    \n",
    "    # 4. Convert similarities tensor back to NumPy array\n",
    "    similarities = similarities_tensor.cpu().numpy()\n",
    "    \n",
    "    # Ensure similarities is a 1D array for np.argsort\n",
    "    if isinstance(similarities, float) or similarities.ndim == 0:\n",
    "        similarities = np.array([similarities])\n",
    "            \n",
    "    num_available_results = len(similarities)\n",
    "    actual_top_n = min(top_n, num_available_results) \n",
    "\n",
    "    # 5. Get indices of top_n similarities in descending order\n",
    "    sorted_indices = np.argsort(similarities)[::-1][:actual_top_n]\n",
    "    \n",
    "    # 6. Retrieve the corresponding original texts and their scores\n",
    "    results = []\n",
    "    for idx in sorted_indices:\n",
    "        results.append((original_texts[idx], similarities[idx]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa072452",
   "metadata": {},
   "source": [
    "## 6. Example Usage (Embeddings, Saving, Loading, and Search)\n",
    "\n",
    "The following cell demonstrates how to use the functions defined above.\n",
    "It shows:\n",
    "1.  Generating an embedding for a single Japanese sentence.\n",
    "2.  Generating embeddings for a batch of multiple Japanese sentences.\n",
    "    *   The function `get_japanese_embedding` handles both single strings and lists of strings automatically.\n",
    "3.  (The cell after this one will show saving/loading and an example of using the search function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668128ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage for generating embeddings\n",
    "print(f\"Using Model: {MODEL_NAME}\\n\")\n",
    "\n",
    "# 1. Single sentence example\n",
    "print(\"--- Single Sentence Embedding ---\")\n",
    "sample_text_single = \"こんにちは、美しい世界！\"\n",
    "embedding_single = get_japanese_embedding(sample_text_single)\n",
    "print(f\"Original sentence: {sample_text_single}\")\n",
    "print(f\"Embedding shape: {embedding_single.shape}\")\n",
    "print(f\"Sample embedding (first 5 values): {embedding_single[:5]}\\n\")\n",
    "\n",
    "# 2. Batch (multiple sentences) example\n",
    "print(\"--- Batch Sentences Embedding ---\")\n",
    "corpus_texts = [\n",
    "    \"これは最初の文です。\",\n",
    "    \"日本語の埋め込みをテストしています。\",\n",
    "    \"これが最後の文になります。\",\n",
    "    \"東京は日本の首都です。\",\n",
    "    \"猫が窓辺で日向ぼっこをしています。\"\n",
    "]\n",
    "corpus_embeddings = get_japanese_embedding(corpus_texts) \n",
    "print(f\"Original corpus sentences: {corpus_texts}\")\n",
    "print(f\"Corpus embeddings shape: {corpus_embeddings.shape}\") \n",
    "if corpus_embeddings.ndim == 2 and corpus_embeddings.shape[0] > 0:\n",
    "    print(f\"Sample embedding for the first corpus sentence (first 5 values): {corpus_embeddings[0, :5]}\\n\")\n",
    "else:\n",
    "    print(\"Could not display sample of corpus embeddings due to unexpected shape.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c8e47",
   "metadata": {},
   "source": [
    "## 7. Saving, Loading, and Search Example\n",
    "\n",
    "This section demonstrates saving and loading the generated batch embeddings, and then using the `search_similar_texts` function with a query to find similar sentences from the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737314d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving, Loading, and Search Example (uses corpus_embeddings and corpus_texts from the cell above)\n",
    "\n",
    "if 'corpus_embeddings' in locals() and isinstance(corpus_embeddings, np.ndarray) and corpus_embeddings.size > 0:\n",
    "    # Saving batch embeddings to a .npy file\n",
    "    print(\"--- Saving Corpus Embeddings ---\")\n",
    "    output_filename = \"japanese_corpus_embeddings.npy\"\n",
    "    np.save(output_filename, corpus_embeddings)\n",
    "    print(f\"Corpus embeddings saved to: {output_filename}\\n\")\n",
    "\n",
    "    # Loading embeddings from the .npy file\n",
    "    print(\"--- Loading Corpus Embeddings ---\")\n",
    "    loaded_corpus_embeddings = np.load(output_filename)\n",
    "    print(f\"Embeddings loaded from: {output_filename}\")\n",
    "    print(f\"Loaded corpus embeddings shape: {loaded_corpus_embeddings.shape}\\n\")\n",
    "\n",
    "    # Example of using the search function\n",
    "    print(\"--- Semantic Search Example (using PyTorch Cosine Similarity) ---\")\n",
    "    query_sentence = \"日本の首都はどこですか。\"\n",
    "    print(f\"Search Query: {query_sentence}\")\n",
    "    \n",
    "    search_results = search_similar_texts(query_sentence, corpus_texts, loaded_corpus_embeddings, top_n=3)\n",
    "    \n",
    "    print(\"Top similar texts found:\")\n",
    "    for text, score in search_results:\n",
    "        print(f\"  Score: {score:.4f} - Text: {text}\")\n",
    "else:\n",
    "    print(\"Skipping saving/loading/search example as 'corpus_embeddings' is not defined, not a NumPy array, or empty. Please run the cell above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
