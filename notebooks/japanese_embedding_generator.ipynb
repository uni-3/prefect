{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b61b89",
   "metadata": {},
   "source": [
    "# Japanese Text Embedding Generator with PLaMo-1B\n",
    "\n",
    "**Goal:** This notebook demonstrates how to generate high-quality text embeddings for Japanese sentences using the `pfnet/plamo-embedding-1b` model from Hugging Face. This model provides specific, optimized methods: `encode_query` for generating embeddings for search queries, and `encode_document` for generating embeddings for documents to be searched against. Utilizing these specialized methods is crucial as they are tailored by the model creators for better performance in retrieval tasks compared to generic sentence embeddings from a base model's forward pass.\n",
    "\n",
    "We will cover:\n",
    "1.  Installing necessary libraries.\n",
    "2.  Loading the pre-trained PLaMo model and its tokenizer (requiring `trust_remote_code=True`).\n",
    "3.  Understanding the model's specific embedding methods (`encode_query`, `encode_document`) and context length.\n",
    "4.  Defining wrapper functions `get_query_embedding` and `get_document_embeddings` that utilize these model-specific methods.\n",
    "5.  Defining utility functions for cosine similarity.\n",
    "6.  Defining a function for semantic search using these specialized embeddings.\n",
    "7.  Running examples for generating query and document embeddings, saving/loading, and performing a search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dae30a",
   "metadata": {},
   "source": [
    "## 1. Library Installation\n",
    "\n",
    "The following libraries are required:\n",
    "*   `torch`: The core PyTorch library for tensor computations and neural network operations.\n",
    "*   `transformers`: Hugging Face's library providing access to pre-trained models (like PLaMo) and tokenizers.\n",
    "*   `sentencepiece`: A tokenizer library often used by models like PLaMo.\n",
    "*   `numpy`: A library for numerical operations, especially for handling the embeddings as arrays.\n",
    "\n",
    "The cell below will install these libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4daaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers sentencepiece numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9df08e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Now, let's import the installed libraries and necessary modules.\n",
    "*   `torch` for tensor operations.\n",
    "*   `torch.nn.functional` (as `F`) for PyTorch functions like cosine similarity.\n",
    "*   `AutoTokenizer` and `AutoModel` from `transformers` for loading the model and tokenizer automatically based on the model name.\n",
    "*   `numpy` (as `np`) for numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee923a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076e822",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model and Tokenizer\n",
    "\n",
    "We will use the `pfnet/plamo-embedding-1b` model, a powerful model specifically trained for generating Japanese text embeddings. This model has custom code defining its own embedding generation methods and thus requires `trust_remote_code=True` during loading.\n",
    "\n",
    "*   **`MODEL_NAME`**: Specifies the Hugging Face model identifier.\n",
    "*   **`AutoTokenizer.from_pretrained(MODEL_NAME)`**: Loads the appropriate tokenizer for the specified model.\n",
    "*   **`AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)`**: Loads the pre-trained PLaMo model. `trust_remote_code=True` is essential here as PLaMo uses custom model code defining methods like `encode_query` and `encode_document`. These methods are often optimized by the model authors for specific tasks (like retrieval) and can yield better performance than generic pooling strategies on the base model's outputs.\n",
    "*   **`device`**: We check if a CUDA-enabled GPU is available and set the device accordingly (`'cuda'` or `'cpu'`).\n",
    "*   **`.to(device)`**: This moves the model's parameters and buffers to the selected device.\n",
    "\n",
    "### Model Context Length and Truncation\n",
    "The PLaMo embedding model (`pfnet/plamo-embedding-1b`) has a maximum context length of **4096 tokens**. \n",
    "Texts longer than this will be truncated by the tokenizer when passed to `encode_document` or `encode_query`.\n",
    "According to the model card, `encode_query` prepends a prefix \"search_query: \" to the input text, which means its effective maximum context length for the user-provided query text is slightly shorter than 4096 tokens to accommodate this prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16773abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "MODEL_NAME = 'pfnet/plamo-embedding-1b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_markdown_query_emb",
   "metadata": {},
   "source": [
    "## 4. Query Embedding Generation Function\n",
    "\n",
    "The function `get_query_embedding` is a wrapper around the model's `encode_query` method. This method is specifically provided and optimized by the PLaMo model creators for encoding search queries, which can lead to better retrieval performance compared to using a generic embedding.\n",
    "\n",
    "**Function Parameters:**\n",
    "*   `query_text` (str): The single Japanese query text.\n",
    "*   `model`: The pre-trained Hugging Face model (already on the target `device`). This model should have an `encode_query` method.\n",
    "*   `tokenizer`: The tokenizer associated with the model.\n",
    "\n",
    "**Process:**\n",
    "1.  Uses `torch.inference_mode()` for efficiency, as gradients are not needed during embedding generation.\n",
    "2.  Calls `model.encode_query(query_text, tokenizer=tokenizer)`. The model's internal `encode_query` method handles tokenization (using the provided tokenizer), moving data to its own device, and generating the embedding tensor.\n",
    "3.  The resulting PyTorch tensor (typically 1D for a single query) is moved to the CPU and converted to a NumPy array.\n",
    "4.  Returns the 1D NumPy array representing the query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_code_query_emb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query_text: str, model, tokenizer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates an embedding for a single query text using model.encode_query.\n",
    "    Assumes model and tokenizer are already configured and model is on the correct device.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'encode_query'):\n",
    "        raise AttributeError(\"The loaded model does not have an 'encode_query' method. Ensure trust_remote_code=True was used and the model supports this.\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        query_embedding_tensor = model.encode_query(query_text, tokenizer=tokenizer)\n",
    "    \n",
    "    return query_embedding_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_markdown_doc_emb",
   "metadata": {},
   "source": [
    "## 5. Document Embeddings Generation Function\n",
    "\n",
    "The function `get_document_embeddings` wraps the model's `encode_document` method. This method is provided and optimized by the PLaMo model creators for encoding documents, such as those that would form a corpus to be searched against. Using this specialized method can improve the quality of document embeddings for retrieval tasks.\n",
    "\n",
    "**Function Parameters:**\n",
    "*   `document_texts` (list of str): A list of Japanese document texts.\n",
    "*   `model`: The pre-trained Hugging Face model (already on the target `device`). This model should have an `encode_document` method.\n",
    "*   `tokenizer`: The tokenizer associated with the model.\n",
    "\n",
    "**Process:**\n",
    "1.  Uses `torch.inference_mode()` for efficiency.\n",
    "2.  Calls `model.encode_document(document_texts, tokenizer=tokenizer)`. The model's internal `encode_document` method handles tokenization for the batch of texts, moving data to its own device, and generating the embedding tensor.\n",
    "3.  The resulting 2D PyTorch tensor (batch_size x embedding_dim) is moved to the CPU and converted to a NumPy array.\n",
    "4.  Returns the 2D NumPy array representing the document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_code_doc_emb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embeddings(document_texts: list[str], model, tokenizer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of document texts using model.encode_document.\n",
    "    Assumes model and tokenizer are already configured and model is on the correct device.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'encode_document'):\n",
    "        raise AttributeError(\"The loaded model does not have an 'encode_document' method. Ensure trust_remote_code=True was used and the model supports this.\")\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        document_embeddings_tensor = model.encode_document(document_texts, tokenizer=tokenizer)\n",
    "        \n",
    "    return document_embeddings_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de3cf8",
   "metadata": {},
   "source": [
    "## Utility Function: Cosine Similarity (NumPy-based)\n",
    "\n",
    "The following function calculates the cosine similarity between two vectors (or a vector and a matrix of vectors) using NumPy. This is a general utility and can be used with any NumPy embeddings, regardless of how they were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e256902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NumPy is imported as np in cell 4)\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between vec_a and vec_b using NumPy.\n",
    "    vec_a: 1D NumPy array.\n",
    "    vec_b: 1D or 2D NumPy array. If 2D, similarity is computed between vec_a and each row of vec_b.\n",
    "    Returns: A scalar float if vec_b was 1D, or a 1D NumPy array of floats if vec_b was 2D.\n",
    "    \"\"\"\n",
    "    vec_a_norm = np.linalg.norm(vec_a)\n",
    "    vec_b_norms = np.linalg.norm(vec_b, axis=-1) \n",
    "\n",
    "    if vec_a_norm == 0:\n",
    "        return 0.0 if vec_b.ndim == 1 else np.zeros(vec_b.shape[0])\n",
    "            \n",
    "    if vec_b.ndim == 1:\n",
    "        dot_prod = np.dot(vec_a, vec_b)\n",
    "        if vec_b_norms == 0: return 0.0\n",
    "        similarities = dot_prod / (vec_a_norm * vec_b_norms)\n",
    "    elif vec_b.ndim == 2:\n",
    "        dot_prod = np.dot(vec_b, vec_a) \n",
    "        similarities = np.zeros(vec_b.shape[0])\n",
    "        non_zero_mask = (vec_b_norms != 0)\n",
    "        if np.any(non_zero_mask):\n",
    "             similarities[non_zero_mask] = dot_prod[non_zero_mask] / (vec_a_norm * vec_b_norms[non_zero_mask])\n",
    "    else:\n",
    "        raise ValueError(\"vec_b must be 1D or 2D array\")\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc69800",
   "metadata": {},
   "source": [
    "## Semantic Search Function (using PyTorch Cosine Similarity)\n",
    "\n",
    "The `search_similar_texts` function implements semantic search. Given a query text, it finds the most similar texts from a provided list of original texts by comparing their embeddings.\n",
    "\n",
    "**Function Parameters:**\n",
    "*   `query_text` (str): The Japanese text to search with.\n",
    "*   `original_texts` (list of str): A list of the original Japanese sentences/texts that correspond to `all_embeddings`.\n",
    "*   `all_embeddings` (2D NumPy array): A NumPy array where each row is an embedding of a text from `original_texts` (these would be document embeddings generated by `get_document_embeddings`).\n",
    "*   `model`, `tokenizer`, `device`: The loaded PLaMo model, tokenizer, and device (needed for `get_query_embedding` and subsequent tensor operations).\n",
    "*   `top_n` (int, optional): The number of top similar texts to return. Defaults to 3.\n",
    "\n",
    "**Process:**\n",
    "1.  Generates an embedding for the `query_text` using the `get_query_embedding` function.\n",
    "2.  Converts the NumPy query embedding and the `all_embeddings` NumPy array (document embeddings) to PyTorch tensors, moving them to the active `device`.\n",
    "3.  Calculates the cosine similarities using `torch.nn.functional.cosine_similarity` for efficient computation between the query tensor and all document tensors.\n",
    "4.  Converts the resulting similarity tensor back to a NumPy array.\n",
    "5.  Identifies the `top_n` texts with the highest similarity scores using NumPy for sorting.\n",
    "6.  Returns a list of tuples, where each tuple contains `(similar_text, similarity_score)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_texts(query_text, original_texts, all_embeddings, model, tokenizer, device, top_n=3):\n",
    "    \"\"\"\n",
    "    Searches for texts in 'original_texts' that are semantically similar to 'query_text'.\n",
    "    Uses torch.nn.functional.cosine_similarity for the calculation.\n",
    "    Args:\n",
    "        query_text (str): The text to search for.\n",
    "        original_texts (list): A list of original text strings.\n",
    "        all_embeddings (np.ndarray): A 2D NumPy array of document embeddings corresponding to original_texts.\n",
    "        model: The pre-trained model.\n",
    "        tokenizer: The tokenizer.\n",
    "        device: The device model is on.\n",
    "        top_n (int): Number of top similar texts to return.\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing (similar_text, similarity_score).\n",
    "    \"\"\"\n",
    "    if not isinstance(all_embeddings, np.ndarray):\n",
    "        all_embeddings = np.array(all_embeddings)\n",
    "    if all_embeddings.ndim != 2:\n",
    "        raise ValueError(f\"all_embeddings must be a 2D array, but got {all_embeddings.ndim} dimensions.\")\n",
    "    if len(original_texts) != all_embeddings.shape[0]:\n",
    "        raise ValueError(f\"Number of original_texts ({len(original_texts)}) must match number of rows in all_embeddings ({all_embeddings.shape[0]}).\")\n",
    "\n",
    "    query_embedding_np = get_query_embedding(query_text, model, tokenizer)\n",
    "    if query_embedding_np.ndim == 2:\n",
    "        query_embedding_np = query_embedding_np.squeeze(0)\n",
    "\n",
    "    query_tensor = torch.from_numpy(query_embedding_np).unsqueeze(0).to(device)\n",
    "    all_embeddings_tensor = torch.from_numpy(all_embeddings).to(device)\n",
    "\n",
    "    similarities_tensor = F.cosine_similarity(query_tensor, all_embeddings_tensor, dim=-1)\n",
    "    similarities = similarities_tensor.cpu().numpy()\n",
    "    \n",
    "    if isinstance(similarities, float) or similarities.ndim == 0:\n",
    "        similarities = np.array([similarities])\n",
    "            \n",
    "    num_available_results = len(similarities)\n",
    "    actual_top_n = min(top_n, num_available_results) \n",
    "\n",
    "    sorted_indices = np.argsort(similarities)[::-1][:actual_top_n]\n",
    "    \n",
    "    results = []\n",
    "    for idx in sorted_indices:\n",
    "        results.append((original_texts[idx], similarities[idx]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa072452",
   "metadata": {},
   "source": [
    "## Example Usage: Generating Query/Document Embeddings, Saving, Loading, and Search\n", # Consolidated title
    "\n",
    "This section demonstrates the complete workflow:\n",
    "1. Generating a query embedding for a single Japanese sentence using `get_query_embedding`.\n",
    "2. Generating document embeddings for a batch of Japanese sentences (our corpus) using `get_document_embeddings`.\n",
    "3. Saving the generated document embeddings to a `.npy` file.\n",
    "4. Loading the embeddings back from the file.\n",
    "5. Performing a semantic search using the `search_similar_texts` function with a new query against the loaded corpus embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668128ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using Model: {MODEL_NAME} on device: {device}\\n\")\n",
    "\n",
    "print(\"--- Generating Query Embedding ---\")\n",
    "sample_query_text = \"こんにちは、美しい世界！\"\n",
    "query_embedding = get_query_embedding(sample_query_text, model, tokenizer)\n",
    "print(f\"Original query: {sample_query_text}\")\n",
    "print(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "print(f\"Sample query embedding (first 5 values): {query_embedding[:5]}\\n\")\n",
    "\n",
    "print(\"--- Generating Document Embeddings for Corpus ---\")\n",
    "corpus_texts = [\n",
    "    \"これは最初の文です。\",\n",
    "    \"日本語の埋め込みをテストしています。\",\n",
    "    \"これが最後の文になります。\",\n",
    "    \"東京は日本の首都です。\",\n",
    "    \"猫が窓辺で日向ぼっこをしています。\"\n",
    "]\n",
    "corpus_embeddings = get_document_embeddings(corpus_texts, model, tokenizer)\n",
    "print(f\"Original corpus sentences: {corpus_texts}\")\n",
    "print(f\"Corpus embeddings shape: {corpus_embeddings.shape}\") \n",
    "if corpus_embeddings.ndim == 2 and corpus_embeddings.shape[0] > 0:\n",
    "    print(f\"Sample embedding for the first corpus sentence (first 5 values): {corpus_embeddings[0, :5]}\\n\")\n",
    "else:\n",
    "    print(\"Could not display sample of corpus embeddings due to unexpected shape.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c8e47",
   "metadata": {},
   "source": [
    "### Saving, Loading, and Search Example (Continued)\n", # Adjusted to flow as continuation
    "\n",
    "Now, we use the `corpus_embeddings` and `corpus_texts` generated above to demonstrate saving, loading, and searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737314d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving, Loading, and Search Example (uses corpus_embeddings and corpus_texts from the cell above)\n",
    "\n",
    "if 'corpus_embeddings' in locals() and isinstance(corpus_embeddings, np.ndarray) and corpus_embeddings.size > 0:\n",
    "    print(\"--- Saving Corpus Embeddings ---\")\n",
    "    output_filename = \"japanese_corpus_embeddings.npy\"\n",
    "    np.save(output_filename, corpus_embeddings)\n",
    "    print(f\"Corpus embeddings saved to: {output_filename}\\n\")\n",
    "\n",
    "    print(\"--- Loading Corpus Embeddings ---\")\n",
    "    loaded_corpus_embeddings = np.load(output_filename)\n",
    "    print(f\"Embeddings loaded from: {output_filename}\")\n",
    "    print(f\"Loaded corpus embeddings shape: {loaded_corpus_embeddings.shape}\\n\")\n",
    "\n",
    "    print(\"--- Semantic Search Example (using PyTorch Cosine Similarity) ---\")\n",
    "    search_query_sentence = \"日本の首都はどこですか。\"\n", # Renamed to avoid conflict with sample_query_text if cells run together
    "    print(f\"Search Query: {search_query_sentence}\")\n",
    "    \n",
    "    search_results = search_similar_texts(search_query_sentence, corpus_texts, loaded_corpus_embeddings, model, tokenizer, device, top_n=3)\n",
    "    \n",
    "    print(\"Top similar texts found:\")\n",
    "    for text, score in search_results:\n",
    "        print(f\"  Score: {score:.4f} - Text: {text}\")\n",
    "else:\n",
    "    print(\"Skipping saving/loading/search example as 'corpus_embeddings' is not defined, not a NumPy array, or empty. Please run the cell above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
